{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "faqg_enerator.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMKYlQ4zzuW6FDiGmibYLVq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wishwagayan/Building-Permit-GO-lang/blob/master/faqg_enerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8-XQWpCl21w"
      },
      "source": [
        "import os\r\n",
        "import sys\r\n",
        "import math\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import spacy\r\n",
        "import re\r\n",
        "import random\r\n",
        "import json\r\n",
        "import en_core_web_sm\r\n",
        "\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqinTAInl7NE",
        "outputId": "97b419d7-8919-46fa-8643-1c412f502a80"
      },
      "source": [
        "!pip install transformers\r\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 35.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 37.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=19cfb2125b17f27e653a736f68637bdf41324aec24d4b90bbeb990e6848abbdc\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jixAD9eCmCXc",
        "outputId": "dcf5cadf-60e7-473a-a899-6bbba4f752a8"
      },
      "source": [
        "!pip install sentencepiece "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 8.5MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGiyr5Hml0tB"
      },
      "source": [
        "from transformers import (\r\n",
        "    AutoTokenizer,\r\n",
        "    AutoModelForSeq2SeqLM,\r\n",
        "    AutoModelForSequenceClassification,\r\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aolXa9OiOPz"
      },
      "source": [
        "\r\n",
        "\r\n",
        "class QuestionGenerator:\r\n",
        "    def __init__(self, model_dir=None):\r\n",
        "\r\n",
        "        QG_PRETRAINED = \"iarfmoose/t5-base-question-generator\"\r\n",
        "        self.ANSWER_TOKEN = \"<answer>\"\r\n",
        "        self.CONTEXT_TOKEN = \"<context>\"\r\n",
        "        self.SEQ_LENGTH = 512\r\n",
        "\r\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "        self.qg_tokenizer = AutoTokenizer.from_pretrained(QG_PRETRAINED, use_fast=False)\r\n",
        "        self.qg_model = AutoModelForSeq2SeqLM.from_pretrained(QG_PRETRAINED)\r\n",
        "        self.qg_model.to(self.device)\r\n",
        "\r\n",
        "        self.qa_evaluator = QAEvaluator(model_dir)\r\n",
        "\r\n",
        "    def generate(\r\n",
        "        self, article, use_evaluator=True, num_questions=None, answer_style=\"all\"\r\n",
        "    ):\r\n",
        "\r\n",
        "        print(\"Generating questions...\\n\")\r\n",
        "\r\n",
        "        qg_inputs, qg_answers = self.generate_qg_inputs(article, answer_style)\r\n",
        "        generated_questions = self.generate_questions_from_inputs(qg_inputs)\r\n",
        "\r\n",
        "        message = \"{} questions doesn't match {} answers\".format(\r\n",
        "            len(generated_questions), len(qg_answers)\r\n",
        "        )\r\n",
        "        assert len(generated_questions) == len(qg_answers), message\r\n",
        "\r\n",
        "        if use_evaluator:\r\n",
        "\r\n",
        "            print(\"Evaluating QA pairs...\\n\")\r\n",
        "\r\n",
        "            encoded_qa_pairs = self.qa_evaluator.encode_qa_pairs(\r\n",
        "                generated_questions, qg_answers\r\n",
        "            )\r\n",
        "            scores = self.qa_evaluator.get_scores(encoded_qa_pairs)\r\n",
        "            if num_questions:\r\n",
        "                qa_list = self._get_ranked_qa_pairs(\r\n",
        "                    generated_questions, qg_answers, scores, num_questions\r\n",
        "                )\r\n",
        "            else:\r\n",
        "                qa_list = self._get_ranked_qa_pairs(\r\n",
        "                    generated_questions, qg_answers, scores\r\n",
        "                )\r\n",
        "\r\n",
        "        else:\r\n",
        "            print(\"Skipping evaluation step.\\n\")\r\n",
        "            qa_list = self._get_all_qa_pairs(generated_questions, qg_answers)\r\n",
        "\r\n",
        "        return qa_list\r\n",
        "\r\n",
        "    def generate_qg_inputs(self, text, answer_style):\r\n",
        "\r\n",
        "        VALID_ANSWER_STYLES = [\"all\", \"sentences\", \"multiple_choice\"]\r\n",
        "\r\n",
        "        if answer_style not in VALID_ANSWER_STYLES:\r\n",
        "            raise ValueError(\r\n",
        "                \"Invalid answer style {}. Please choose from {}\".format(\r\n",
        "                    answer_style, VALID_ANSWER_STYLES\r\n",
        "                )\r\n",
        "            )\r\n",
        "\r\n",
        "        inputs = []\r\n",
        "        answers = []\r\n",
        "\r\n",
        "        if answer_style == \"sentences\" or answer_style == \"all\":\r\n",
        "            segments = self._split_into_segments(text)\r\n",
        "            for segment in segments:\r\n",
        "                sentences = self._split_text(segment)\r\n",
        "                prepped_inputs, prepped_answers = self._prepare_qg_inputs(\r\n",
        "                    sentences, segment\r\n",
        "                )\r\n",
        "                inputs.extend(prepped_inputs)\r\n",
        "                answers.extend(prepped_answers)\r\n",
        "\r\n",
        "        if answer_style == \"multiple_choice\" or answer_style == \"all\":\r\n",
        "            sentences = self._split_text(text)\r\n",
        "            prepped_inputs, prepped_answers = self._prepare_qg_inputs_MC(sentences)\r\n",
        "            inputs.extend(prepped_inputs)\r\n",
        "            answers.extend(prepped_answers)\r\n",
        "\r\n",
        "        return inputs, answers\r\n",
        "\r\n",
        "    def generate_questions_from_inputs(self, qg_inputs):\r\n",
        "        generated_questions = []\r\n",
        "\r\n",
        "        for qg_input in qg_inputs:\r\n",
        "            question = self._generate_question(qg_input)\r\n",
        "            generated_questions.append(question)\r\n",
        "\r\n",
        "        return generated_questions\r\n",
        "\r\n",
        "    def _split_text(self, text):\r\n",
        "        MAX_SENTENCE_LEN = 128\r\n",
        "\r\n",
        "        sentences = re.findall(\".*?[.!\\?]\", text)\r\n",
        "\r\n",
        "        cut_sentences = []\r\n",
        "        for sentence in sentences:\r\n",
        "            if len(sentence) > MAX_SENTENCE_LEN:\r\n",
        "                cut_sentences.extend(re.split(\"[,;:)]\", sentence))\r\n",
        "        # temporary solution to remove useless post-quote sentence fragments\r\n",
        "        cut_sentences = [s for s in sentences if len(s.split(\" \")) > 5]\r\n",
        "        sentences = sentences + cut_sentences\r\n",
        "\r\n",
        "        return list(set([s.strip(\" \") for s in sentences]))\r\n",
        "\r\n",
        "    def _split_into_segments(self, text):\r\n",
        "        MAX_TOKENS = 490\r\n",
        "\r\n",
        "        paragraphs = text.split(\"\\n\")\r\n",
        "        tokenized_paragraphs = [\r\n",
        "            self.qg_tokenizer(p)[\"input_ids\"] for p in paragraphs if len(p) > 0\r\n",
        "        ]\r\n",
        "\r\n",
        "        segments = []\r\n",
        "        while len(tokenized_paragraphs) > 0:\r\n",
        "            segment = []\r\n",
        "            while len(segment) < MAX_TOKENS and len(tokenized_paragraphs) > 0:\r\n",
        "                paragraph = tokenized_paragraphs.pop(0)\r\n",
        "                segment.extend(paragraph)\r\n",
        "            segments.append(segment)\r\n",
        "        return [self.qg_tokenizer.decode(s) for s in segments]\r\n",
        "\r\n",
        "    def _prepare_qg_inputs(self, sentences, text):\r\n",
        "        inputs = []\r\n",
        "        answers = []\r\n",
        "\r\n",
        "        for sentence in sentences:\r\n",
        "            qg_input = \"{} {} {} {}\".format(\r\n",
        "                self.ANSWER_TOKEN, sentence, self.CONTEXT_TOKEN, text\r\n",
        "            )\r\n",
        "            inputs.append(qg_input)\r\n",
        "            answers.append(sentence)\r\n",
        "\r\n",
        "        return inputs, answers\r\n",
        "\r\n",
        "    def _prepare_qg_inputs_MC(self, sentences):\r\n",
        "\r\n",
        "        spacy_nlp = en_core_web_sm.load()\r\n",
        "        docs = list(spacy_nlp.pipe(sentences, disable=[\"parser\"]))\r\n",
        "        inputs_from_text = []\r\n",
        "        answers_from_text = []\r\n",
        "\r\n",
        "        for i in range(len(sentences)):\r\n",
        "            entities = docs[i].ents\r\n",
        "            if entities:\r\n",
        "                for entity in entities:\r\n",
        "                    qg_input = \"{} {} {} {}\".format(\r\n",
        "                        self.ANSWER_TOKEN, entity, self.CONTEXT_TOKEN, sentences[i]\r\n",
        "                    )\r\n",
        "                    answers = self._get_MC_answers(entity, docs)\r\n",
        "                    inputs_from_text.append(qg_input)\r\n",
        "                    answers_from_text.append(answers)\r\n",
        "\r\n",
        "        return inputs_from_text, answers_from_text\r\n",
        "\r\n",
        "    def _get_MC_answers(self, correct_answer, docs):\r\n",
        "\r\n",
        "        entities = []\r\n",
        "        for doc in docs:\r\n",
        "            entities.extend([{\"text\": e.text, \"label_\": e.label_} for e in doc.ents])\r\n",
        "\r\n",
        "        # remove duplicate elements\r\n",
        "        entities_json = [json.dumps(kv) for kv in entities]\r\n",
        "        pool = set(entities_json)\r\n",
        "        num_choices = (\r\n",
        "            min(4, len(pool)) - 1\r\n",
        "        )  # -1 because we already have the correct answer\r\n",
        "\r\n",
        "        # add the correct answer\r\n",
        "        final_choices = []\r\n",
        "        correct_label = correct_answer.label_\r\n",
        "        final_choices.append({\"answer\": correct_answer.text, \"correct\": True})\r\n",
        "        pool.remove(\r\n",
        "            json.dumps({\"text\": correct_answer.text, \"label_\": correct_answer.label_})\r\n",
        "        )\r\n",
        "\r\n",
        "        # find answers with the same NER label\r\n",
        "        matches = [e for e in pool if correct_label in e]\r\n",
        "\r\n",
        "        # if we don't have enough then add some other random answers\r\n",
        "        if len(matches) < num_choices:\r\n",
        "            choices = matches\r\n",
        "            pool = pool.difference(set(choices))\r\n",
        "            choices.extend(random.sample(pool, num_choices - len(choices)))\r\n",
        "        else:\r\n",
        "            choices = random.sample(matches, num_choices)\r\n",
        "\r\n",
        "        choices = [json.loads(s) for s in choices]\r\n",
        "        for choice in choices:\r\n",
        "            final_choices.append({\"answer\": choice[\"text\"], \"correct\": False})\r\n",
        "        random.shuffle(final_choices)\r\n",
        "        return final_choices\r\n",
        "\r\n",
        "    def _generate_question(self, qg_input):\r\n",
        "        self.qg_model.eval()\r\n",
        "        encoded_input = self._encode_qg_input(qg_input)\r\n",
        "        with torch.no_grad():\r\n",
        "            output = self.qg_model.generate(input_ids=encoded_input[\"input_ids\"])\r\n",
        "        question = self.qg_tokenizer.decode(output[0], skip_special_tokens=True)\r\n",
        "        return question\r\n",
        "\r\n",
        "    def _encode_qg_input(self, qg_input):\r\n",
        "        return self.qg_tokenizer(\r\n",
        "            qg_input,\r\n",
        "            padding='max_length',\r\n",
        "            max_length=self.SEQ_LENGTH,\r\n",
        "            truncation=True,\r\n",
        "            return_tensors=\"pt\",\r\n",
        "        ).to(self.device)\r\n",
        "\r\n",
        "    def _get_ranked_qa_pairs(\r\n",
        "        self, generated_questions, qg_answers, scores, num_questions=10\r\n",
        "    ):\r\n",
        "        if num_questions > len(scores):\r\n",
        "            num_questions = len(scores)\r\n",
        "            print(\r\n",
        "                \"\\nWas only able to generate {} questions. For more questions, please input a longer text.\".format(\r\n",
        "                    num_questions\r\n",
        "                )\r\n",
        "            )\r\n",
        "\r\n",
        "        qa_list = []\r\n",
        "        for i in range(num_questions):\r\n",
        "            index = scores[i]\r\n",
        "            qa = self._make_dict(\r\n",
        "                generated_questions[index].split(\"?\")[0] + \"?\", qg_answers[index]\r\n",
        "            )\r\n",
        "            qa_list.append(qa)\r\n",
        "        return qa_list\r\n",
        "\r\n",
        "    def _get_all_qa_pairs(self, generated_questions, qg_answers):\r\n",
        "        qa_list = []\r\n",
        "        for i in range(len(generated_questions)):\r\n",
        "            qa = self._make_dict(\r\n",
        "                generated_questions[i].split(\"?\")[0] + \"?\", qg_answers[i]\r\n",
        "            )\r\n",
        "            qa_list.append(qa)\r\n",
        "        return qa_list\r\n",
        "\r\n",
        "    def _make_dict(self, question, answer):\r\n",
        "        qa = {}\r\n",
        "        qa[\"question\"] = question\r\n",
        "        qa[\"answer\"] = answer\r\n",
        "        return qa\r\n",
        "\r\n",
        "\r\n",
        "class QAEvaluator:\r\n",
        "    def __init__(self, model_dir=None):\r\n",
        "\r\n",
        "        QAE_PRETRAINED = \"iarfmoose/bert-base-cased-qa-evaluator\"\r\n",
        "        self.SEQ_LENGTH = 512\r\n",
        "\r\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "        self.qae_tokenizer = AutoTokenizer.from_pretrained(QAE_PRETRAINED)\r\n",
        "        self.qae_model = AutoModelForSequenceClassification.from_pretrained(\r\n",
        "            QAE_PRETRAINED\r\n",
        "        )\r\n",
        "        self.qae_model.to(self.device)\r\n",
        "\r\n",
        "    def encode_qa_pairs(self, questions, answers):\r\n",
        "        encoded_pairs = []\r\n",
        "        for i in range(len(questions)):\r\n",
        "            encoded_qa = self._encode_qa(questions[i], answers[i])\r\n",
        "            encoded_pairs.append(encoded_qa.to(self.device))\r\n",
        "        return encoded_pairs\r\n",
        "\r\n",
        "    def get_scores(self, encoded_qa_pairs):\r\n",
        "        scores = {}\r\n",
        "        self.qae_model.eval()\r\n",
        "        with torch.no_grad():\r\n",
        "            for i in range(len(encoded_qa_pairs)):\r\n",
        "                scores[i] = self._evaluate_qa(encoded_qa_pairs[i])\r\n",
        "\r\n",
        "        return [\r\n",
        "            k for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)\r\n",
        "        ]\r\n",
        "\r\n",
        "    def _encode_qa(self, question, answer):\r\n",
        "        if type(answer) is list:\r\n",
        "            for a in answer:\r\n",
        "                if a[\"correct\"]:\r\n",
        "                    correct_answer = a[\"answer\"]\r\n",
        "        else:\r\n",
        "            correct_answer = answer\r\n",
        "        return self.qae_tokenizer(\r\n",
        "            text=question,\r\n",
        "            text_pair=correct_answer,\r\n",
        "            padding=\"max_length\",\r\n",
        "            max_length=self.SEQ_LENGTH,\r\n",
        "            truncation=True,\r\n",
        "            return_tensors=\"pt\",\r\n",
        "        )\r\n",
        "\r\n",
        "    def _evaluate_qa(self, encoded_qa_pair):\r\n",
        "        output = self.qae_model(**encoded_qa_pair)\r\n",
        "        return output[0][0][1]\r\n",
        "\r\n",
        "\r\n",
        "def print_qa(qa_list, show_answers=True):\r\n",
        "    for i in range(len(qa_list)):\r\n",
        "        space = \" \" * int(np.where(i < 9, 3, 4))  # wider space for 2 digit q nums\r\n",
        "\r\n",
        "        print(\"{}) Q: {}\".format(i + 1, qa_list[i][\"question\"]))\r\n",
        "\r\n",
        "        answer = qa_list[i][\"answer\"]\r\n",
        "\r\n",
        "        # print a list of multiple choice answers\r\n",
        "        if type(answer) is list:\r\n",
        "\r\n",
        "            if show_answers:\r\n",
        "                print(\r\n",
        "                    \"{}A: 1.\".format(space),\r\n",
        "                    answer[0][\"answer\"],\r\n",
        "                    np.where(answer[0][\"correct\"], \"(correct)\", \"\"),\r\n",
        "                )\r\n",
        "                for j in range(1, len(answer)):\r\n",
        "                    print(\r\n",
        "                        \"{}{}.\".format(space + \"   \", j + 1),\r\n",
        "                        answer[j][\"answer\"],\r\n",
        "                        np.where(answer[j][\"correct\"] == True, \"(correct)\", \"\"),\r\n",
        "                    )\r\n",
        "\r\n",
        "            else:\r\n",
        "                print(\"{}A: 1.\".format(space), answer[0][\"answer\"])\r\n",
        "                for j in range(1, len(answer)):\r\n",
        "                    print(\"{}{}.\".format(space + \"   \", j + 1), answer[j][\"answer\"])\r\n",
        "            print(\"\")\r\n",
        "\r\n",
        "        # print full sentence answers\r\n",
        "        else:\r\n",
        "            if show_answers:\r\n",
        "                print(\"{}A:\".format(space), answer, \"\\n\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOcXjCIlmEN5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}